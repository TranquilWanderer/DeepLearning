{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47caf8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Users\\AA66\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "# from tf_utils import load_dataset, random_mini_batches, convert_to_one_hot, predict\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0193ff67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "y_hat = tf.constant(36, name='y_hat')            # Define y_hat constant. Set to 36.\n",
    "y = tf.constant(39, name='y')                    # Define y. Set to 39\n",
    "\n",
    "init = tf.Variable(initial_value=0.0)             # Initializes the variables\n",
    "loss = tf.Variable((y - y_hat)**2, name='loss')  # Create a variable for the loss\n",
    "\n",
    "print(loss.numpy())                               # Prints the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d540c3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(2)\n",
    "b = tf.constant(10)\n",
    "c = tf.multiply(a,b)\n",
    "print(c.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ff44a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "tensorflow 1.x\n",
    "# Change the value of x in the feed_dict\n",
    "\n",
    "x = tf.placeholder(tf.int64, name = 'x')\n",
    "print(sess.run(2 * x, feed_dict = {x: 3}))\n",
    "sess.close()\n",
    "\"\"\"\n",
    "# tensorflow 2.x\n",
    "import tensorflow as tf\n",
    "\n",
    "x = tf.constant(3, dtype=tf.int64)\n",
    "result = 2 * x\n",
    "\n",
    "print(result.numpy())  # Execute the operation and print the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73e96db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_function\n",
    "\n",
    "def linear_function():\n",
    "    \"\"\"\n",
    "    Implements a linear function: \n",
    "            Initializes W to be a random tensor of shape (4,3)\n",
    "            Initializes X to be a random tensor of shape (3,1)\n",
    "            Initializes b to be a random tensor of shape (4,1)\n",
    "    Returns: \n",
    "    result -- runs the session for Y = WX + b \n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    ### START CODE HERE ### (4 lines of code)  \n",
    "    X = tf.constant(np.random.randn(3,1), name = \"X\")  \n",
    "    W = tf.constant(np.random.randn(4,3), name = \"W\")  \n",
    "    b = tf.constant(np.random.randn(4,1), name = \"b\")  \n",
    "    Y = tf.add(tf.matmul(W,X),b)  \n",
    "    ### END CODE HERE ###   \n",
    "      \n",
    "\n",
    "    return Y.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b83c876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result = [[-2.15657382]\n",
      " [ 2.95891446]\n",
      " [-1.08926781]\n",
      " [-0.84538042]]\n"
     ]
    }
   ],
   "source": [
    "print( \"result = \" + str(linear_function()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff3f3ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Computes the sigmoid of z\n",
    "    \n",
    "    Arguments:\n",
    "    z -- input value, scalar or vector\n",
    "    \n",
    "    Returns: \n",
    "    results -- the sigmoid of z\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### ( approx. 4 lines of code)  \n",
    "  \n",
    "    # Compute sigmoid(z)\n",
    "    sigmoid = tf.sigmoid(z)\n",
    "  \n",
    "    \n",
    "    ### END CODE HERE ###  \n",
    "    \n",
    "    return sigmoid.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ae7fc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(0) = 0.5\n",
      "sigmoid(12) = 0.99999386\n"
     ]
    }
   ],
   "source": [
    "print (\"sigmoid(0) = \" + str(sigmoid(0.)))\n",
    "print (\"sigmoid(12) = \" + str(sigmoid(12.)))  #注意输入必须是浮点数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ed1988a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: cost\n",
    "\n",
    "def cost(logits, labels):\n",
    "    \"\"\"\n",
    "    Computes the cost using the sigmoid cross entropy\n",
    "    \n",
    "    Arguments:\n",
    "    logits -- vector containing z, output of the last linear unit (before the final sigmoid activation)\n",
    "    labels -- vector of labels y (1 or 0) \n",
    "    \n",
    "    Note: What we've been calling \"z\" and \"y\" in this class are respectively called \"logits\" and \"labels\" \n",
    "    in the TensorFlow documentation. So logits will feed into z, and labels into y. \n",
    "    \n",
    "    Returns:\n",
    "    cost -- runs the session of the cost (formula (2))\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###   \n",
    "    \n",
    "    # Convert logits and labels to double tensors\n",
    "    logits_double = tf.cast(logits, dtype=tf.float64)\n",
    "    labels_double = tf.cast(labels, dtype=tf.float64)  \n",
    "    \n",
    "    # Use the loss function (approx. 1 line)  \n",
    "    cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_double,labels=labels_double)  \n",
    "      \n",
    "    ### END CODE HERE ###  \n",
    "    \n",
    "    return cost.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7572805e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = [1.00538722 1.03664083 0.41385432 0.39956614]\n"
     ]
    }
   ],
   "source": [
    "logits = sigmoid(np.array([0.2,0.4,0.7,0.9]))\n",
    "cost = cost(logits, np.array([0,0,1,1]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd7e320",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
